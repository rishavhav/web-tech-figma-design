<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@800&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="assets/css/styles.css" />
    <title>Rishav's AI Blog</title>
    <style></style>
  </head>
  <body>
    <div style="display: flex; flex-direction: column">
      <header>
        <nav class="navbar">
          <a href="index.html" class="nav-link">Homepage</a>
          <div class="underline"></div>
        </nav>
      </header>

      <div class="blog-header">
        <div class="blog-title">Rishav’s AI blog</div>
      </div>

      <h1 class="heading" style="padding-left: 20px">How AI Bias Can Unconsciously Influence Humans</h1>

      <div id="content" style="margin: 20px auto; width: 97%; height: 100%; color: #111111; font-size: 18px; font-family: Poppins; font-weight: 500; line-height: 28px; word-wrap: break-word">
        <div>The rapid advancement of artificial intelligence over the past decade has led to AI systems being deployed across various sectors, from healthcare to criminal justice. However, as AI continues permeating critical areas, there are rising concerns over algorithmic biases and how they might negatively impact end users. A recent study published in Scientific American highlights how humans can unwittingly absorb prejudices from biased AI systems and retain this bias long after they stop interacting with these algorithms. The research paper titled “Humans absorb bias from AI and keep it after they stop using the algorithm” underscores experimental evidence on how people’s judgments and decisions can be influenced by algorithmic biases without them realizing it. The authors of the study asked over 200 participants to take on the role of a judge reviewing real defendants’ risk assessment scores generated by a widely used recidivism algorithm called COMPAS. However, the participants were split into groups and shown scores with different levels of bias against black defendants versus white defendants.</div>
        <div style="display: flex; width: 100%; height: 50%; margin-top: 20px">
          <div style="width: 500px; height: 500px; background: #c4c4c4"></div>
          <div id="second-paragh" style="flex: 1; margin-left: 20px">After viewing several hundred cases, the participants were asked to assess new defendants without seeing the algorithm's scores. Shockingly, those who were originally shown biased COMPAS scores continued exhibiting biases in line with the skewed scores they had previously seen. Essentially, the biases from the algorithmic scoring system had crept into the participants' decision-making process, unconsciously impacting how they evaluated new cases based on race. This critical finding drives home the point that biased AI systems can fundamentally alter human judgment in subtle but impactful ways. People may absorb prejudices or unfairness from algorithms designed inconsiderately and retain this bias within their mental models, applying it in future decisions. So even removing access to a visibly biased algorithm may not eliminate its influence over people’s opinions. The risks here span across sectors where AI guides significant decisions, whether healthcare, employment, criminal justice or banking. If algorithms contain problematic biases like racial or gender prejudice, over time, they could skew human perspectives and undermine objectives of fairness and equality. And unfortunately, once thinking patterns drift, they become challenging to correct.</div>
        </div>

        <div style="display: block; margin-top: 20px">So how do we tackle this threat of AI bias creeping into human judgments? Firstly, those developing AI systems, especially for sensitive use-cases, should rigorously audit for biases during design, testing and monitoring after deployment. Teams should evaluate model outputs across different demographic groups to catch uneven accuracy or error rates early. Secondly, where risks of significant harm exist, obtaining diverse feedback on system functionality could flag potential issues. Lastly, better education around inherent human biases and improved AI literacy among user groups can help lessen unconscious absorption of unfairness. The scientific study on COMPAS algorithm bias absorption makes an alarming point – biased AI can covertly make us more biased over time. So technology creators and businesses employing AI have an ethical obligation to proactively address prejudice risks in automated systems. With advanced algorithms increasingly acting as aids for major personal and societal decisions, ensuring they align with ideals of equality and impartiality protects not just individuals subject to their outputs but also wider human judgment from shifting negatively. If we fail tackling AI bias, we jeopardize fairness across communities.</div>

        <a href="aboutMe.html"><button style="display: block; margin-left: 20px">About me.</button></a>

        <div style="margin-top: 20px" id="newPostHeader">Other Posts you may like</div>

        <div id="boxContainer">
          <a href="blogPage1.html" class="box" style="color: black; text-decoration: none">
            <div>Embracing the Future...</div>
          </a>

          <a href="blogPage2.html" class="box" style="color: black; text-decoration: none">
            <div>Navigating the World...</div>
          </a>

          <a href="blogPage4.html" class="box" style="color: black; text-decoration: none">
            <div>Navigating the Road...</div>
          </a>
        </div>
      </div>

      <div class="clearfix"></div>

      <footer>
        <div id="footer-bg">
          <div style="display: flex; flex-direction: column">
            <div id="footer-about">About</div>
            <a href="aboutMe.html" id="footer-about-me-link">About me</a>
          </div>
          <div style="display: flex; flex-direction: column">
            <div style="width: 100%; height: 100%; opacity: 0.25; color: white; font-size: 12px; font-family: Poppins; font-weight: 500; line-height: 30px; letter-spacing: 0.6px; word-wrap: break-word">FOLLOW RISHAV’S</div>
          </div>
        </div>
      </footer>
    </div>
    <script src="script.js"></script>
  </body>
</html>
